{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c53842-34cc-4605-886b-a4ff8d3d4610",
   "metadata": {},
   "source": [
    "# Training Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9875fc76-ef90-411e-9500-3f9a94c520b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a606f2b8-547f-4e6f-95e8-8b5e1fe10966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cnn_module as cnn\n",
    "import importlib\n",
    "importlib.reload(cnn)\n",
    "# Load and Preprocess the minist dataset\n",
    "train_data, validation_data, test_data = cnn.preprocess_mnist()\n",
    "\n",
    "\n",
    "# Step decay schedule\n",
    "def step_decay_schedule(initial_lr=0.001, decay_factor=0.5, step_size=5):\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** (epoch // step_size))\n",
    "    return schedule\n",
    "\n",
    "# Exponential decay schedule\n",
    "def exponential_decay_schedule(initial_lr=0.001, decay_rate=0.95):\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_rate ** epoch)\n",
    "    return schedule\n",
    "\n",
    "# Create and train a CNN model \n",
    "def create_and_train_model(learning_rate=0.001):\n",
    "    model = keras.Sequential([\n",
    "        # First Convolutional Block\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', name='conv1', \n",
    "                           input_shape=(28, 28, 1)),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), name='pool1'),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), name='pool2'),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), name='pool3'),\n",
    "        \n",
    "        # Flatten the output to feed into dense layers\n",
    "        keras.layers.Flatten(name='flatten'),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        keras.layers.Dense(128, activation='relu', name='dense1'),\n",
    "        keras.layers.Dropout(0.5, name='dropout'),  # Dropout for regularization\n",
    "        \n",
    "        # Output layer\n",
    "        keras.layers.Dense(10, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Step decay: reduce learning rate after fixed intervals\n",
    "    lr_scheduler = LearningRateScheduler(\n",
    "            step_decay_schedule(initial_lr=learning_rate),\n",
    "            verbose=1\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Exponential decay: continuously reduce learning rate\n",
    "    lr_scheduler = LearningRateScheduler(\n",
    "        exponential_decay_schedule(initial_lr=learning_rate),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Add early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model with a small subset for demonstration\n",
    "    validation_inputs, validation_targets = next(iter(validation_data))\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=20,\n",
    "        #batch_size=100,\n",
    "        validation_data=(validation_inputs, validation_targets),\n",
    "        callbacks=[lr_scheduler, early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0cab1a69-b2fe-4afa-a633-1845fa8ebb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/20\n",
      "540/540 - 19s - loss: 0.2997 - accuracy: 0.9049 - val_loss: 0.0637 - val_accuracy: 0.9785 - lr: 0.0010 - 19s/epoch - 36ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.00095.\n",
      "Epoch 2/20\n",
      "540/540 - 19s - loss: 0.0834 - accuracy: 0.9755 - val_loss: 0.0432 - val_accuracy: 0.9865 - lr: 9.5000e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009025.\n",
      "Epoch 3/20\n",
      "540/540 - 18s - loss: 0.0585 - accuracy: 0.9831 - val_loss: 0.0271 - val_accuracy: 0.9912 - lr: 9.0250e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.000857375.\n",
      "Epoch 4/20\n",
      "540/540 - 18s - loss: 0.0466 - accuracy: 0.9871 - val_loss: 0.0266 - val_accuracy: 0.9918 - lr: 8.5737e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0008145062499999999.\n",
      "Epoch 5/20\n",
      "540/540 - 18s - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0224 - val_accuracy: 0.9928 - lr: 8.1451e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0007737809374999998.\n",
      "Epoch 6/20\n",
      "540/540 - 18s - loss: 0.0318 - accuracy: 0.9905 - val_loss: 0.0177 - val_accuracy: 0.9942 - lr: 7.7378e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0007350918906249999.\n",
      "Epoch 7/20\n",
      "540/540 - 18s - loss: 0.0278 - accuracy: 0.9915 - val_loss: 0.0169 - val_accuracy: 0.9942 - lr: 7.3509e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0006983372960937497.\n",
      "Epoch 8/20\n",
      "540/540 - 18s - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0113 - val_accuracy: 0.9965 - lr: 6.9834e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0006634204312890623.\n",
      "Epoch 9/20\n",
      "540/540 - 18s - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.0101 - val_accuracy: 0.9965 - lr: 6.6342e-04 - 18s/epoch - 34ms/step\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0006302494097246091.\n",
      "Epoch 10/20\n",
      "540/540 - 19s - loss: 0.0160 - accuracy: 0.9951 - val_loss: 0.0115 - val_accuracy: 0.9965 - lr: 6.3025e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0005987369392383787.\n",
      "Epoch 11/20\n",
      "540/540 - 19s - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0066 - val_accuracy: 0.9982 - lr: 5.9874e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0005688000922764596.\n",
      "Epoch 12/20\n",
      "540/540 - 19s - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.0068 - val_accuracy: 0.9982 - lr: 5.6880e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0005403600876626366.\n",
      "Epoch 13/20\n",
      "540/540 - 19s - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.0067 - val_accuracy: 0.9978 - lr: 5.4036e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0005133420832795048.\n",
      "Epoch 14/20\n",
      "540/540 - 19s - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.0056 - val_accuracy: 0.9987 - lr: 5.1334e-04 - 19s/epoch - 36ms/step\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00048767497911552955.\n",
      "Epoch 15/20\n",
      "540/540 - 19s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0050 - val_accuracy: 0.9982 - lr: 4.8767e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.000463291230159753.\n",
      "Epoch 16/20\n",
      "540/540 - 19s - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0039 - val_accuracy: 0.9988 - lr: 4.6329e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00044012666865176535.\n",
      "Epoch 17/20\n",
      "540/540 - 19s - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0063 - val_accuracy: 0.9977 - lr: 4.4013e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0004181203352191771.\n",
      "Epoch 18/20\n",
      "540/540 - 19s - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0039 - val_accuracy: 0.9985 - lr: 4.1812e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0003972143184582182.\n",
      "Epoch 19/20\n",
      "540/540 - 19s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0038 - val_accuracy: 0.9992 - lr: 3.9721e-04 - 19s/epoch - 35ms/step\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00037735360253530727.\n",
      "Epoch 20/20\n",
      "540/540 - 19s - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0028 - val_accuracy: 0.9990 - lr: 3.7735e-04 - 19s/epoch - 35ms/step\n"
     ]
    }
   ],
   "source": [
    "model, history = create_and_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2cc9f551-bec0-4f99-bf1a-51ad6d5a475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.002839036>,\n",
       " 'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.999>}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_metrics_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8a5f804c-e220-4e34-af87-7e1c4fe46db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 965ms/step - loss: 0.0258 - accuracy: 0.9943\n",
      "\n",
      "Final model test accuracy: 0.9943\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data, verbose=1)\n",
    "print(f\"\\nFinal model test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3a7a6c5a-1099-4441-9ba9-37c47e7d7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#model.save('temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc8b43e-e9a1-4cb6-8b6e-171eb35c3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"mnist_web_app\\models\\exponential_cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119ef474-e619-4581-a7bd-a0eb062c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cedfd0-a4f1-4132-9b0e-701b386945e6",
   "metadata": {},
   "source": [
    "## 1. Visualize Filter Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e348165f-d2f9-47f7-a178-95cd2ae63061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize Filter Weights from First Layer\n",
    "def visualize_filters(model):\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = 'visuals'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the weights from the first convolutional layer\n",
    "    filters, biases = model.layers[0].get_weights()\n",
    "    \n",
    "    # Normalize filter values to 0-1 for visualization\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    # Plot the filters\n",
    "    n_filters = filters.shape[3]\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        ax = axes[i // 8, i % 8]\n",
    "        ax.imshow(filters[:,:,0,i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Conv1 Layer Filters', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'filters'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6befabb1-216e-4cff-8194-9aa17a4f3ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\josep\\\\Documents\\\\Projects\\\\Github\\\\MNIST\\\\visuals\\\\filters.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_filters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mvisualize_filters\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConv1 Layer Filters\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvisuals/filters.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1022\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m-> 1023\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\figure.py:3378\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3375\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m   3376\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m-> 3378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 458\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\matplotlib\\image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1687\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1688\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1689\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF2.0\\lib\\site-packages\\PIL\\Image.py:2563\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2561\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2563\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\josep\\\\Documents\\\\Projects\\\\Github\\\\MNIST\\\\visuals\\\\filters.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_filters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0d4f5-9564-4cdd-87c5-107e853a41ea",
   "metadata": {},
   "source": [
    "Filter Visualization Analysis:\n",
    "\n",
    "- The first convolutional layer learns basic feature detectors like edges, curves, and textures\n",
    "- Lighter areas show where the filter activates (positive values)\n",
    "- Darker areas show where the filter suppresses (negative values)\n",
    "- These filters are the foundation for all higher-level features detected in deeper layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054d9a6-040b-482d-9603-77c01e2673e3",
   "metadata": {},
   "source": [
    "## 2. Feature Maps/Activations Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e1258-8bb2-44ff-a447-800a847bfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Maps/Activations Visualization\n",
    "def visualize_feature_maps(model, img_idx=0, output_dir='visuals'):\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get a test image\n",
    "    img = x_test[img_idx:img_idx+1]\n",
    "    \n",
    "    # Create models to extract activations from different layers\n",
    "    activation_models = {}\n",
    "    for layer_name in ['conv1', 'pool1', 'conv2', 'pool2', 'conv3', 'pool3']:\n",
    "        activation_models[layer_name] = Model(\n",
    "            inputs=model.input,\n",
    "            outputs=model.get_layer(layer_name).output\n",
    "        )\n",
    "    \n",
    "    # Get activations for the test image\n",
    "    activations = {}\n",
    "    for layer_name, activation_model in activation_models.items():\n",
    "        activations[layer_name] = activation_model.predict(img)\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(x_test[img_idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Original Image (Digit: {y_test[img_idx]})', fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, 'original_digit.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Display activations for each layer\n",
    "    for layer_name, activation in activations.items():\n",
    "        n_features = activation.shape[3]\n",
    "        size = activation.shape[1]\n",
    "        \n",
    "        # Calculate grid dimensions based on number of features\n",
    "        grid_size = int(np.ceil(np.sqrt(n_features)))\n",
    "        \n",
    "        # Display up to 64 feature maps (8x8 grid)\n",
    "        n_cols = min(grid_size, 12)\n",
    "        n_rows = min((n_features + n_cols - 1) // n_cols, 12)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 2*n_rows))\n",
    "        fig.suptitle(f'Feature Maps: {layer_name}', fontsize=18)\n",
    "        \n",
    "        for i in range(n_rows * n_cols):\n",
    "            if i < n_features:\n",
    "                if n_rows > 1:\n",
    "                    ax = axes[i // n_cols, i % n_cols]\n",
    "                else:\n",
    "                    ax = axes[i % n_cols]\n",
    "                ax.imshow(activation[0, :, :, i], cmap='viridis')\n",
    "                ax.set_title(f'Filter {i}')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                if n_rows > 1:\n",
    "                    axes[i // n_cols, i % n_cols].axis('off')\n",
    "                else:\n",
    "                    axes[i % n_cols].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        filename = f'{layer_name}_activations.png'\n",
    "        plt.savefig(os.path.join(output_dir, filename))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d2877-e3b7-4e24-a9e5-7c01007ab1af",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 2. Feature Maps/Activations Visualization\n",
    "def visualize_feature_maps(model, x_test, y_test, img_idx=0, output_dir='visuals'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get a test image\n",
    "    img = x_test[img_idx:img_idx+1]\n",
    "\n",
    "    # Create models to extract activations from different layers\n",
    "    layer_names = ['conv1', 'pool1', 'conv2', 'pool2', 'conv3', 'pool3']\n",
    "    activation_models = {\n",
    "        name: Model(inputs=model.input, outputs=model.get_layer(name).output)\n",
    "        for name in layer_names\n",
    "    }\n",
    "\n",
    "    # Get activations\n",
    "    activations = {\n",
    "        name: model_fn.predict(img)\n",
    "        for name, model_fn in activation_models.items()\n",
    "    }\n",
    "\n",
    "    # Save original image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(x_test[img_idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Original Image (Digit: {y_test[img_idx]})', fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, 'original_digit.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Save activation maps\n",
    "    for layer_name, activation in activations.items():\n",
    "        n_features = activation.shape[-1]\n",
    "        grid_size = int(np.ceil(np.sqrt(n_features)))\n",
    "        n_cols = min(grid_size, 12)\n",
    "        n_rows = min((n_features + n_cols - 1) // n_cols, 12)\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 2*n_rows))\n",
    "        fig.suptitle(f'Feature Maps: {layer_name}', fontsize=18)\n",
    "\n",
    "        for i in range(n_rows * n_cols):\n",
    "            ax = axes[i // n_cols, i % n_cols] if n_rows > 1 else axes[i % n_cols]\n",
    "            if i < n_features:\n",
    "                ax.imshow(activation[0, :, :, i], cmap='viridis')\n",
    "                ax.set_title(f'Filter {i}')\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        filename = f'{layer_name}_activations.png'\n",
    "        plt.savefig(os.path.join(output_dir, filename))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17d154-1706-48e5-890b-5a981b032523",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4e131-61e3-43c2-aa59-67bb713cab50",
   "metadata": {},
   "source": [
    "Feature Map Analysis:\n",
    "\n",
    "- Early layers (conv1, pool1) detect basic features like edges and corners\n",
    "- Later layers (conv2, pool2) combine these to detect more complex patterns\n",
    "- Bright areas show where the filter strongly activated for that specific feature\n",
    "- Different feature maps activate for different aspects of the digit's shape\n",
    "- Some feature maps remain mostly inactive, suggesting they specialize in features not present in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644c0b5-0549-4bb1-966c-133346f66cd7",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a4dc-be2c-4c3d-aa74-5387088f489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Confusion Matrix Visualization\n",
    "def visualize_confusion_matrix():\n",
    "    # Generate predictions for test data\n",
    "    predictions = model.predict(x_test)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('visuals/confusion_matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most confused digit pairs\n",
    "    off_diag = np.where(~np.eye(10, dtype=bool))\n",
    "    confused_pairs = [(i, j, cm[i, j]) for i, j in zip(off_diag[0], off_diag[1])]\n",
    "    confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"Most confused digit pairs (true, predicted, count):\")\n",
    "    for true, pred, count in confused_pairs[:5]:\n",
    "        print(f\"  {true} misclassified as {pred}: {count} times\")\n",
    "    \n",
    "    # Visualize examples of confused digits\n",
    "    top_confused = confused_pairs[0]  # The most confused pair\n",
    "    true_digit, pred_digit, _ = top_confused\n",
    "    \n",
    "    # Find indices of the confused examples\n",
    "    confused_indices = np.where((y_test == true_digit) & (y_pred == pred_digit))[0]\n",
    "    \n",
    "    if len(confused_indices) > 0:\n",
    "        # Show a few examples\n",
    "        n_examples = min(5, len(confused_indices))\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(n_examples):\n",
    "            plt.subplot(1, n_examples, i+1)\n",
    "            plt.imshow(x_test[confused_indices[i]].reshape(28, 28), cmap='gray')\n",
    "            plt.title(f'True: {true_digit}, Pred: {pred_digit}')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visuals/confusion.png')\n",
    "        plt.show()\n",
    "\n",
    "    return true_digit, pred_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d0ae6-1e4c-4d81-83eb-6ab14505f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_digit, pred_digit = visualize_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59176b5-89ce-4585-abf9-89862dac7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "    Confusion Matrix Analysis:\n",
    "    \n",
    "    - Most common confusion: Digit {true_digit} misclassified as {pred_digit}\n",
    "    - This confusion often occurs because these digits share similar visual features\n",
    "    - The model achieves high overall accuracy, but struggles with specific digit pairs\n",
    "    - Examining these confused examples helps understand model limitations\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127de95e-7285-45dd-a2a8-298897459a58",
   "metadata": {},
   "source": [
    "## 4. Class Activation Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd6e86-b949-4832-9b91-57f932adb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_activation_fixed(model, image_path, img_idx=12):\n",
    "    \"\"\"Visualize what parts of the image the model is focusing on for classification\"\"\"\n",
    "    # Get a test image\n",
    "    img = x_test[img_idx:img_idx+1]\n",
    "    true_label = y_test[img_idx]\n",
    "    \n",
    "    # Create a model that outputs the activations from the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv3')\n",
    "    conv_model = tf.keras.Model(inputs=model.input, outputs=last_conv_layer.output)\n",
    "    \n",
    "    # Get intermediate dense layer\n",
    "    dense_layer = model.get_layer('dense1')\n",
    "    dense_model = tf.keras.Model(inputs=model.input, outputs=dense_layer.output)\n",
    "    \n",
    "    # Get the conv activations\n",
    "    conv_activations = conv_model.predict(img)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    \n",
    "    # Get the gradient of the predicted class with respect to the conv layer output\n",
    "    grad_model = tf.keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[conv_model.output, model.output]\n",
    "    )\n",
    "    \n",
    "    # Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get both the conv activations and predictions\n",
    "        conv_output, predictions = grad_model(img)\n",
    "        # Get the loss for the predicted class\n",
    "        loss = predictions[:, predicted_class]\n",
    "    \n",
    "    # Get the gradients with respect to the conv output\n",
    "    grads = tape.gradient(loss, conv_output)\n",
    "    \n",
    "    # Global average pooling on the gradients (gradient-weighted class activation map)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Create the heatmap by multiplying each channel by its gradient importance\n",
    "    heatmap = tf.zeros_like(conv_output[0, :, :, 0])\n",
    "    for i in range(pooled_grads.shape[0]):\n",
    "        heatmap += conv_output[0, :, :, i] * pooled_grads[i]\n",
    "    \n",
    "    # For visualization, we normalize the heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    # Resize the heatmap to the original image size\n",
    "    heatmap_resized = tf.image.resize(\n",
    "        tf.expand_dims(tf.expand_dims(heatmap, axis=0), axis=-1),\n",
    "        (28, 28)\n",
    "    ).numpy().squeeze()\n",
    "    \n",
    "    # Display the results\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(img.reshape(28, 28), cmap='gray')\n",
    "    ax1.set_title(f'Original (Digit: {true_label})', fontsize=18)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # CAM heatmap\n",
    "    ax2.imshow(heatmap_resized, cmap='jet')\n",
    "    ax2.set_title(f'Class Activation Map for Digit {predicted_class}', fontsize=18)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Overlay heatmap on original image\n",
    "    ax3.imshow(img.reshape(28, 28), cmap='gray')\n",
    "    ax3.imshow(heatmap_resized, cmap='jet', alpha=0.5)\n",
    "    ax3.set_title('Overlay', fontsize=18)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_path)\n",
    "    plt.show()\n",
    "\n",
    "    return true_label, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238eb59-69f1-4b1a-a7a0-763091623ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the fixed CAM visualization on a few different digits\n",
    "image_path = ['visuals/CAM1.png', 'visuals/CAM2.png', 'visuals/CAM3.png']\n",
    "for path in image_path:\n",
    "    try:\n",
    "        true_label, predicted_class = visualize_class_activation_fixed(model, image_path=path, img_idx=np.random.randint(0, len(x_test)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Try with a known good index\n",
    "        print(\"Trying with a default index instead...\")\n",
    "        true_label, predicted_class = visualize_class_activation_fixed(model, img_idx=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106f226-c6f4-4f76-b674-610642c386f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "    Class Activation Map Analysis:\n",
    "    \n",
    "    - True label: {true_label}, Predicted label: {predicted_class}\n",
    "    - The heatmap shows which parts of the image were most important for the model's classification\n",
    "    - Warmer colors (red/yellow) indicate areas with higher activation/importance\n",
    "    - Cooler colors (blue) show areas the model largely ignored\n",
    "    - The model focuses most on the distinctive features of the digit\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b313d-a328-4b8f-bb4a-61d3038f5325",
   "metadata": {},
   "source": [
    "## 5. t-SNE Visualization of Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54215e-7f42-4978-a4ae-475e409a5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. t-SNE Visualization of Feature Space\n",
    "def visualize_embedding():\n",
    "    print(\"Generating t-SNE visualization of the feature space...\")\n",
    "    \n",
    "    # Create a model that outputs features from the dense layer before classification\n",
    "    feature_model = Model(inputs=model.input, outputs=model.get_layer('dense1').output)\n",
    "    \n",
    "    # Extract features for a subset of test images (for speed)\n",
    "    sample_size = 2000\n",
    "    indices = np.random.choice(len(x_test), sample_size, replace=False)\n",
    "    features = feature_model.predict(x_test[indices])\n",
    "    labels = y_test[indices]\n",
    "    \n",
    "    # Apply t-SNE dimensionality reduction\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plot the 2D t-SNE embeddings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        features_2d[:, 0], features_2d[:, 1], \n",
    "        c=labels, cmap='tab10', \n",
    "        alpha=0.7, s=10\n",
    "    )\n",
    "    plt.colorbar(scatter, ticks=range(10))\n",
    "    plt.title('t-SNE Visualization of MNIST Digit Features', fontsize=18)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.savefig('visuals/tsne.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317020a-501d-46c7-b036-1f12bba46e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb5f48-9d0c-4839-88fc-c464e4e80b92",
   "metadata": {},
   "source": [
    "t-SNE Visualization Analysis:\n",
    "    \n",
    "- Each point represents a digit image projected into 2D space\n",
    "- Colors indicate the digit class (0-9)\n",
    "- Clustered points show digits that the model represents similarly in its feature space\n",
    "- Well-separated clusters indicate the model effectively distinguishes between these digit classes\n",
    "- Overlapping clusters suggest the model has difficulty separating these digit classes\n",
    "- This visualization helps understand how the model's internal representation groups similar digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409e260-d2be-4e48-9be4-b9e0715d5988",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472587a-2771-4a50-b340-be134ee98bb1",
   "metadata": {},
   "source": [
    "#### Why Visualization and Analysis Matter for Neural Networks:\n",
    "\n",
    "1. Interpretability: Visualizations help understand what features the model is learning \n",
    "   and how it makes decisions, turning the \"black box\" into a more understandable system.\n",
    "\n",
    "2. Debugging and Improvement: Identifying where and why the model makes mistakes allows \n",
    "   targeted improvements to the architecture or training process.\n",
    "\n",
    "3. Feature Understanding: Visualizing filters and activations reveals what patterns the \n",
    "   network recognizes at different layers, from simple edges to complex digit shapes.\n",
    "\n",
    "4. Validation: Confirming that the model is learning meaningful features rather than \n",
    "   exploiting dataset quirks or shortcuts increases confidence in its generalizability.\n",
    "\n",
    "5. Education: These visualizations provide intuitive understanding of how CNNs work, \n",
    "   making deep learning concepts more accessible.\n",
    "\n",
    "For MNIST specifically:\n",
    "- Filter visualizations show how the network learns to detect edges and curves critical for digit recognition\n",
    "- Confusion matrices reveal which digits are most commonly confused (often 4/9, 3/8, and 5/3)\n",
    "- Activation maps illustrate how the network progressively builds more complex representations of digits\n",
    "- Class activation maps show which parts of the image are most important for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9009d7-478a-4250-b3eb-1a2ff0bf99a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.0]",
   "language": "python",
   "name": "conda-env-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

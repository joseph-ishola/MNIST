{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d19a3e-6451-48e4-a204-22a7a8ba021b",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1d957-97b3-4ecd-a22a-acc50ce0260a",
   "metadata": {},
   "source": [
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook).\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "Our goal would be to build a neural network with hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a290581c-3c33-44bc-bfd7-162be4bab87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07801b-acbf-4404-84e8-1db42038bb0d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07dc9683-b063-4770-aa43-23c6f9ab9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from tfds\n",
    "mnist_datasets, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "486b1414-e9a2-4d52-b194-7856b59c9847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='C:\\\\Users\\\\josep\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the info of the mnist datasets\n",
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d127b916-eee5-4b33-8c6b-a2ae5d57226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the mnist datasets\n",
    "mnist_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4db47b-d601-427f-887f-84ceae303bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have loaded the dataset, we can easily extract the training and testing dataset\n",
    "mnist_train, mnist_test = mnist_datasets['train'], mnist_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63783cd9-9e31-4dcd-b860-f2805ddd8bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=60000>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of training data in the mnist datasets\n",
    "tf.cast(mnist_info.splits['train'].num_examples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567ca6c-af9e-40ff-81a2-1e91930e70db",
   "metadata": {},
   "source": [
    "By default, TF has training and testing datasets but no validation sets,\n",
    "thus we must split it on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d66f241c-2474-49e6-bc38-474cd1ad1c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=6000>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cadad0e1-59b7-46db-8e32-8f49ad6da972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10000>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's also store the number of test samples in a dedicated variable\n",
    "num_test_samples = tf.cast(mnist_info.splits['test'].num_examples, tf.int64)\n",
    "num_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80573d55-e43c-470d-902f-cdb48a4c27fa",
   "metadata": {},
   "source": [
    "Normally, we would like to scale our data in some way to make the result more numerically stable. \\\n",
    "In this case we will simply prefer to have inputs between 0 and 1.\\\n",
    "The method dataset.map(custom_function) allows us to apply a custom transformation to a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c533f2d-08a6-4877-a156-a84eaa20df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32) #float values\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# we will apply the scaling transformation on the mnist_train and store it as both train and validation data\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "# same thing for the test data so it has the same magnitude as the train and validation\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c626c-b061-490e-8d2a-3a02b53b02f5",
   "metadata": {},
   "source": [
    "We will also shuffle the combine train and validation data. \\\n",
    "There is no need to shuffle the test data, because we won't be training on it.\n",
    "\n",
    "This BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets,\n",
    "then we can't shuffle the whole dataset in one go because we can't fit it all in memory, so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them.\n",
    "* if BUFFER_SIZE = 1 -- no shuffling will actually happen\n",
    "* if BUFFER_SIZE >= num samples -- shuffling is uniform\n",
    "* BUFFER_SIZE in between -- a computational optimization to approximate uniform shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4851f4d-744d-4362-b81b-3265fd51a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# there is a shuffle method readily available and we just need to specify the buffer size\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# batch the data\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "test_data = scaled_test_data.batch(num_test_samples)\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc550c33-50f6-4577-b7e1-42a3303159e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched train dataset\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb89522-0fec-4902-a680-0f5f326b700c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### DNN using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb9f5f-3504-4211-8324-01c07d8ff619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.0]",
   "language": "python",
   "name": "conda-env-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
